{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ファイルの読み込み\n",
    "df_c = pd.read_csv('outputC_3.csv', sep=';')\n",
    "df_d = pd.read_csv('outputD_3.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_cにのみ存在するカラム: {'隣'}\n",
      "df_dにのみ存在するカラム: {'ワーシャルフロイド', 'XOR', 'union', 'UF', '素数', 'セグ', '素因', '二進', 'エラトステネス', '和が'}\n"
     ]
    }
   ],
   "source": [
    "def combine_columns_containing_keyword(df, keyword):\n",
    "    # キーワードを含むカラム名のリスト\n",
    "    columns_with_keyword = [col for col in df.columns if keyword in col]\n",
    "    \n",
    "    # これらのカラムの値を合計する\n",
    "    df[keyword] = df[columns_with_keyword].fillna(0).sum(axis=1)\n",
    "    \n",
    "    # キーワードを含む元のカラムを削除\n",
    "    df.drop(columns=columns_with_keyword, inplace=True)\n",
    "\n",
    "# '隣'という文字を含むカラムを統合\n",
    "combine_columns_containing_keyword(df_c, '隣')\n",
    "combine_columns_containing_keyword(df_d, '隣')\n",
    "\n",
    "# 不要なカラムの削除\n",
    "columns_to_drop = ['二人', '前進', '全く', '使い分け', '項目', '線分', '各項', '最善', '事項', '素早く', '全般', '最長', '引き分け', '最新', '深め', 'キューブ', '右辺', '等分', '分数', '調和', '分線', '分野']\n",
    "df_c.drop(columns=columns_to_drop, inplace=True)\n",
    "columns_to_drop = ['項数', '分裂', '進行', '全然', '何分', '完全', '余分', '精進', '進ん', 'ランプ', '全員', '色分け', '多項', '分布', '最右', '分別', '多分', '分岐', 'ベルトランチェビシェフ', '塗り分け', '進む', '分間', '最上', '全問', '大幅', '進み', 'ランダム', '分かれる']\n",
    "df_d.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# df_cにのみ存在するカラム\n",
    "unique_columns_c = set(df_c.columns) - set(df_d.columns)\n",
    "\n",
    "# df_dにのみ存在するカラム\n",
    "unique_columns_d = set(df_d.columns) - set(df_c.columns)\n",
    "\n",
    "print(\"df_cにのみ存在するカラム:\", unique_columns_c)\n",
    "print(\"df_dにのみ存在するカラム:\", unique_columns_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "統合が完了しました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2939/2961190357.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['classifier'] = np.nan\n"
     ]
    }
   ],
   "source": [
    "# 共通のカラムがない場合はエラー\n",
    "if len(set(df_c.columns) & set(df_d.columns)) == 0:\n",
    "    raise ValueError(\"共通のカラムがありません。結合キーを指定してください。\")\n",
    "\n",
    "# 共通カラムに基づいてデータフレームを水平方向に結合\n",
    "merged_df = pd.merge(df_c, df_d, how='outer')\n",
    "\n",
    "\n",
    "# 片方にしかないカラムの集合\n",
    "unique_columns_c = set(df_c.columns) - set(df_d.columns)\n",
    "unique_columns_d = set(df_d.columns) - set(df_c.columns)\n",
    "\n",
    "# df_cにしかないカラムのNaNを0に変更\n",
    "for col in unique_columns_c:\n",
    "    merged_df[col].fillna(0, inplace=True)\n",
    "\n",
    "# df_dにしかないカラムのNaNを0に変更\n",
    "for col in unique_columns_d:\n",
    "    merged_df[col].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# 'classifier'カラムを追加し、すべての値をNaNで初期化\n",
    "merged_df['classifier'] = np.nan\n",
    "\n",
    "\n",
    "print(\"統合が完了しました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359, 148)"
      ]
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_list = ['全点', '分かっ', '分かれ', '初項', '最近', '最適', '最速', '分かり', '分かる', '差分', '全て', '深く', '辺り', '分配', '全体', '進め', '素直', '青木', '分母', '分子', '分ける', '左辺', '自分', '分け', '両辺', '最終', '最低', '最悪', '時分', '全部', '分離', '最も', '分から', '分割', '半分', '最初', '最後', '要素', '進塁', '最', '分類', '各辺', 'C', '和が', '項']\n",
    "merged_df.drop(columns=delete_list, inplace=True)\n",
    "\n",
    "# '全'と'探索'の両方が正である場合、'classifier'カラムに1を設定\n",
    "merged_df.loc[(merged_df['全'] > 0) & (merged_df['探索'] > 0), 'classifier'] = 1\n",
    "\n",
    "merged_df.loc[(merged_df['包'] > 0) & (merged_df['原理'] > 0), 'classifier'] = 14\n",
    "\n",
    "# '幅', '優先', '探索'がすべて正であるか、'dfs'または'BFS'が正である行に対して、'classifier'カラムに7を設定\n",
    "merged_df.loc[((merged_df['幅'] > 0) & (merged_df['優先'] > 0) & (merged_df['探索'] > 0)) | (merged_df['bfs'] > 0) | (merged_df['BFS'] > 0), 'classifier'] = 7\n",
    "\n",
    "# '深', '優先', '探索'がすべて正であるか、'dfs'または'DFS'が正である行に対して、'classifier'カラムに7を設定\n",
    "merged_df.loc[((merged_df['深'] > 0) & (merged_df['優先'] > 0) & (merged_df['探索'] > 0)) | (merged_df['dfs'] > 0) | (merged_df['DFS'] > 0), 'classifier'] = 15\n",
    "\n",
    "# '尺取り'または'尺取'が正である行に対して、'classifier'カラムに6を設定\n",
    "merged_df.loc[(merged_df['尺取り'] > 0) | (merged_df['尺取'] > 0), 'classifier'] = 6\n",
    "\n",
    "# 上記のいずれかのカラムが正である行に対して、'classifier'カラムに10を設定\n",
    "merged_df.loc[(merged_df['ユークリッド'] > 0) | (merged_df['互除'] > 0) | (merged_df['公約'] > 0) | (merged_df['約数'] > 0) | (merged_df['gcd'] > 0) | (merged_df['GCD'] > 0), 'classifier'] = 10\n",
    "\n",
    "# 上記のいずれかのカラムが正である行に対して、'classifier'カラムに2を設定\n",
    "merged_df.loc[((merged_df['累積'] > 0) & (merged_df['和'] > 0)) | (merged_df['単調'] > 0) | (merged_df['imos'] > 0) | (merged_df['lower_bound'] > 0), 'classifier'] = 2\n",
    "\n",
    "# 上記のいずれかのカラムが正である行に対して、'classifier'カラムに3を設定\n",
    "merged_df.loc[((merged_df['二'] > 0) & (merged_df['分'] > 0) & (merged_df['探索'] > 0)) | (merged_df['分木'] > 0), 'classifier'] = 3\n",
    "\n",
    "# 'ラン'、'レングス'、'圧縮'のすべてが正である場合、または'座標'と'圧縮'の両方が正である場合、'classifier'カラムに4を設定\n",
    "merged_df.loc[((merged_df['ラン'] > 0) & (merged_df['レングス'] > 0)) | ((merged_df['座標'] > 0) & (merged_df['圧縮'] > 0)), 'classifier'] = 4\n",
    "\n",
    "# '連結'、'成分'、'分解'のすべてが正であるか、'Find'、'UF'、'union'のいずれかが正である場合、'classifier'カラムに8を設定\n",
    "merged_df.loc[((merged_df['連結'] > 0) & (merged_df['成分'] > 0) & (merged_df['分解'] > 0)) | (merged_df['Find'] > 0) | (merged_df['UF'] > 0) | (merged_df['union'] > 0), 'classifier'] = 8\n",
    "\n",
    "# '動的'と'計画'の両方が正であるか、'DP'または'dp'が正である場合、'classifier'カラムに5を設定\n",
    "merged_df.loc[((merged_df['動的'] > 0) & (merged_df['計画'] > 0)) | (merged_df['DP'] > 0) | (merged_df['dp'] > 0), 'classifier'] = 5\n",
    "\n",
    "# '最短'と'経路'の両方が正であるか、'貪欲'または'ワーシャルフロイド'が正である場合、'classifier'カラムに9を設定\n",
    "merged_df.loc[((merged_df['最短'] > 0) & (merged_df['経路'] > 0)) | (merged_df['貪欲'] > 0) | (merged_df['ワーシャルフロイド'] > 0), 'classifier'] = 9\n",
    "\n",
    "# '論理'または'XOR'のカラムが正である場合、'classifier'カラムに13を設定\n",
    "merged_df.loc[(merged_df['論理'] > 0) | (merged_df['XOR'] > 0), 'classifier'] = 13\n",
    "\n",
    "# 'セグ'または'セグメント'と'木'が正である場合、'classifier'カラムに12を設定\n",
    "merged_df.loc[((merged_df['セグ'] > 0) | (merged_df['セグメント'] > 0)) & (merged_df['木'] > 0), 'classifier'] = 12\n",
    "\n",
    "# 'mod'または'エラトステネス'または'素数'が正である場合、'classifier'カラムに11を設定\n",
    "merged_df.loc[(merged_df['mod'] > 0) | (merged_df['エラトステネス'] > 0) | (merged_df['素数'] > 0), 'classifier'] = 11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['全', '探索', '包', '原理', '幅', '優先', 'bfs', 'BFS', '深', 'dfs', 'DFS', '尺取り', '尺取',\n",
    "                     'ユークリッド', '互除', '公約', '約数', 'gcd', 'GCD', '累積', '和', '単調', 'imos', 'lower_bound',\n",
    "                     '二', '分', '分木', 'ラン', 'レングス', '座標', '圧縮', '連結', '成分', '分解', 'Find', 'UF', 'union',\n",
    "                     '動的', '計画', 'DP', 'dp', '最短', '経路', '貪欲', 'ワーシャルフロイド', '論理', 'XOR', 'セグ', 'セグメント',\n",
    "                     '木', 'mod', 'エラトステネス', '素数']\n",
    "\n",
    "cols = [col for col in merged_df.columns if col != 'Title']\n",
    "merged_df[cols] = merged_df[cols].astype(float)\n",
    "\n",
    "merged_df = merged_df.drop(columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359, 50)"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 削除するべきTitleのリスト\n",
    "# titles_to_remove = ['C178', 'C179', 'C183', 'C193', 'C240', 'D186', 'D218', 'D307']\n",
    "\n",
    "# # Titleカラムの値が上記リストに含まれる行を削除\n",
    "# merged_df = merged_df[~merged_df['Title'].isin(titles_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('merged_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各カラムにおける欠損値の数:\n",
      "Title               0\n",
      "頂点                  0\n",
      "時計                  0\n",
      "総和                  0\n",
      "最高                  0\n",
      "素                   0\n",
      "区間                  0\n",
      "遷移                  0\n",
      "最小                  0\n",
      "キュー                 0\n",
      "ハッシュ                0\n",
      "最大                  0\n",
      "クエリ                 0\n",
      "部分                  0\n",
      "進                   0\n",
      "グラフ                 0\n",
      "十分                  0\n",
      "辺                   0\n",
      "倍数                  0\n",
      "回転                  0\n",
      "再帰                  0\n",
      "if                  0\n",
      "abs                 0\n",
      "top                 0\n",
      "find                0\n",
      "rep                 0\n",
      "length              0\n",
      "max                 0\n",
      "push                0\n",
      "empty               0\n",
      "sort                0\n",
      "swap                0\n",
      "%                   0\n",
      "map                 0\n",
      "priority_queue      0\n",
      "pop                 0\n",
      "bit                 0\n",
      "tuple               0\n",
      "min                 0\n",
      "while               0\n",
      "push_back           0\n",
      "Union               0\n",
      "vector              0\n",
      "que                 0\n",
      "reverse             0\n",
      "size                0\n",
      "隣                   0\n",
      "二進                  0\n",
      "素因                  0\n",
      "classifier        136\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 各カラムにおける欠損値の数を確認\n",
    "missing_values = merged_df.isnull().sum()\n",
    "print(\"各カラムにおける欠損値の数:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title列を除外\n",
    "tfidf_df = merged_df.drop(columns=['Title', 'classifier'])\n",
    "\n",
    "# TFの計算\n",
    "epsilon = 1e-10\n",
    "tf = tfidf_df.div(tfidf_df.sum(axis=1) + epsilon, axis=0)\n",
    "\n",
    "# IDFの計算\n",
    "idf = np.log((1 + len(tfidf_df)) / (1 + (tfidf_df > 0).sum())) + 1\n",
    "\n",
    "# TF-IDFの計算\n",
    "tfidf = tf * idf\n",
    "\n",
    "# 結果を元のDataFrameに追加\n",
    "merged_df[tfidf.columns] = tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "print((tfidf_df.sum(axis=1) == 0).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各カラムにおける欠損値の数:\n",
      "Title               0\n",
      "頂点                  0\n",
      "時計                  0\n",
      "総和                  0\n",
      "最高                  0\n",
      "素                   0\n",
      "区間                  0\n",
      "遷移                  0\n",
      "最小                  0\n",
      "キュー                 0\n",
      "ハッシュ                0\n",
      "最大                  0\n",
      "クエリ                 0\n",
      "部分                  0\n",
      "進                   0\n",
      "グラフ                 0\n",
      "十分                  0\n",
      "辺                   0\n",
      "倍数                  0\n",
      "回転                  0\n",
      "再帰                  0\n",
      "if                  0\n",
      "abs                 0\n",
      "top                 0\n",
      "find                0\n",
      "rep                 0\n",
      "length              0\n",
      "max                 0\n",
      "push                0\n",
      "empty               0\n",
      "sort                0\n",
      "swap                0\n",
      "%                   0\n",
      "map                 0\n",
      "priority_queue      0\n",
      "pop                 0\n",
      "bit                 0\n",
      "tuple               0\n",
      "min                 0\n",
      "while               0\n",
      "push_back           0\n",
      "Union               0\n",
      "vector              0\n",
      "que                 0\n",
      "reverse             0\n",
      "size                0\n",
      "隣                   0\n",
      "二進                  0\n",
      "素因                  0\n",
      "classifier        136\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 各カラムにおける欠損値の数を確認\n",
    "missing_values = merged_df.isnull().sum()\n",
    "print(\"各カラムにおける欠損値の数:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('TfIdf_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifierが欠損していないものは訓練データ\n",
    "train_data = merged_df[merged_df['classifier'].notna()]\n",
    "\n",
    "# classifierが欠損しているものは予測データ\n",
    "predict_data = merged_df[merged_df['classifier'].isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier\n",
       "5.0     38\n",
       "1.0     34\n",
       "2.0     27\n",
       "3.0     24\n",
       "9.0     22\n",
       "11.0    19\n",
       "10.0    12\n",
       "15.0    12\n",
       "8.0     10\n",
       "7.0      7\n",
       "4.0      5\n",
       "12.0     5\n",
       "13.0     5\n",
       "6.0      2\n",
       "14.0     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['classifier'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "欠損値を持つカラム:\n",
      "classifier    136\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "predict_data.drop(columns=['classifier'])\n",
    "missing_values = merged_df.isnull().sum()\n",
    "missing_columns = missing_values[missing_values > 0]\n",
    "print(\"\\n欠損値を持つカラム:\")\n",
    "print(missing_columns)\n",
    "\n",
    "# missing_rows = predict_data[predict_data.isnull().any(axis=1)]\n",
    "# print(\"\\n欠損値を持つ行:\")\n",
    "# print(missing_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2939/46074647.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_data['classifier'] = predictions\n",
      "/tmp/ipykernel_2939/46074647.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['Title'] = title_train\n",
      "/tmp/ipykernel_2939/46074647.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_data['Title'] = title_predict\n"
     ]
    }
   ],
   "source": [
    "# Titleカラムの保存\n",
    "title_train = train_data['Title']\n",
    "title_predict = predict_data['Title']\n",
    "\n",
    "# 特徴量とラベルの分割\n",
    "X_train = train_data.drop(columns=['classifier', 'Title'])\n",
    "y_train = train_data['classifier']\n",
    "\n",
    "# モデルの訓練\n",
    "classifier_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier_model.fit(X_train, y_train)\n",
    "\n",
    "# 予測用データの特徴量（'Title'も削除）\n",
    "X_predict = predict_data.drop(columns=['classifier', 'Title'])\n",
    "\n",
    "# 予測\n",
    "predictions = classifier_model.predict(X_predict)\n",
    "check = classifier_model.predict(X_train)\n",
    "\n",
    "# 予測結果をDataFrameに追加\n",
    "predict_data['classifier'] = predictions\n",
    "\n",
    "# Titleカラムを再結合\n",
    "train_data['Title'] = title_train\n",
    "predict_data['Title'] = title_predict\n",
    "\n",
    "# 訓練データと予測データを再結合\n",
    "final_df = pd.concat([train_data, predict_data])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "# タイトルの最後の三文字（数字部分）と最初の一文字（CまたはD）を取り出して並び替え\n",
    "final_df['title_order'] = final_df['Title'].apply(lambda x: (x[0], int(x[1:])))\n",
    "final_df = final_df.sort_values(by='title_order')\n",
    "\n",
    "# title_orderカラムを削除して元のデータフレームの形に戻す\n",
    "final_df = final_df.drop(columns=['title_order'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('final_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測精度: 96.86%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_train, check)\n",
    "print(f\"予測精度: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
